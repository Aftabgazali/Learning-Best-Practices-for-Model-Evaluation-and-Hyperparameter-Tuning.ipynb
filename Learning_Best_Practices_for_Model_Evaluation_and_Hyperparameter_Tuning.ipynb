{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMDgx1f28vHXe8oBU2nqFHO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aftabgazali/Learning-Best-Practices-for-Model-Evaluation-and-Hyperparameter-Tuning.ipynb/blob/main/Learning_Best_Practices_for_Model_Evaluation_and_Hyperparameter_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the Breast Cancer Library"
      ],
      "metadata": {
        "id": "hl1HWfTq4MqT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po1fFJhE3OOT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "breast_data = load_breast_cancer()\n",
        "df = pd.DataFrame(data = breast_data.data, columns = breast_data.feature_names)\n",
        "df['target'] = breast_data.target\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cEKnw7414ePE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Class labels are {np.unique(df['target'])}\")"
      ],
      "metadata": {
        "id": "s9TB_QZP5FGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "fCFYOX4g46rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the dataset into Training & Testing"
      ],
      "metadata": {
        "id": "GLsd9-Aq5WDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)"
      ],
      "metadata": {
        "id": "9UP78rPc5asV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipelining"
      ],
      "metadata": {
        "id": "rFqbfjuQ5qNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The make_pipeline function takes an arbitrary number of scikit-learn transformers (objects that sup-\n",
        "port the fit and transform methods as input), followed by a scikit-learn estimator that implements the\n",
        "fit and predict methods. In our preceding code example, we provided two scikit-learn transformers,\n",
        "StandardScaler and PCA, and a LogisticRegression estimator as inputs to the make_pipeline func-\n",
        "tion, which constructs a scikit-learn Pipeline object from these objects.\n",
        "We can think of a scikit-learn Pipeline as a meta-estimator or wrapper around those individual\n",
        "transformers and estimators. If we call the fit method of Pipeline, the data will be passed down a\n",
        "series of transformers via fit and transform calls on these intermediate steps until it arrives at the\n",
        "estimator object (the final element in a pipeline). The estimator will then be fitted to the transformed\n",
        "training data.\n",
        "\n",
        "* When we executed the fit method on the pipe_lr pipeline in the preceding code example,\n",
        "StandardScaler first performed fit and transform calls on the training data. Second, the trans-\n",
        "formed training data was passed on to the next object in the pipeline, PCA. Similar to the previous\n",
        "step, PCA also executed fit and transform on the scaled input data and passed it to the final element\n",
        "of the pipeline, the estimator.\n",
        "\n",
        "* Finally, the LogisticRegression estimator was fit to the training data after it underwent transfor-\n",
        "mations via StandardScaler and PCA. Again, we should note that there is no limit to the number of\n",
        "intermediate steps in a pipeline; however, if we want to use the pipeline for prediction tasks, the last\n",
        "pipeline element has to be an estimator."
      ],
      "metadata": {
        "id": "MUECF8rJ6tCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "pipeline = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression())\n",
        "pipeline.fit(X_train,y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "test_acc = pipeline.score(X_test, y_test)\n",
        "print(f\"Testing Accuracy {test_acc*100:.2f}\")"
      ],
      "metadata": {
        "id": "dNy7ryGc5i8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing the Model's Performance"
      ],
      "metadata": {
        "id": "REaCmrQm8vdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Best Approach is to use k-fold cross-validation, we randomly split the training dataset into k folds without replacement.\n",
        "Here, k â€“ 1 folds, the so-called training folds, are used for the model training, and one fold, the so-called\n",
        "test fold, is used for performance evaluation. This procedure is repeated k times so that we obtain k\n",
        "models and performance estimates*\n",
        "\n",
        "**Working on bigger dataset, `k=10` value is generally a good estimate.**"
      ],
      "metadata": {
        "id": "3xRAhjjB8x3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(estimator=pipeline, X=X_train, y = y_train, cv=10)\n",
        "print(f\"CV accuracy scores | {scores*100}\")\n",
        "print(f\"\\nCV accuracy {np.mean(scores)*100:.2f}\")"
      ],
      "metadata": {
        "id": "etsl_5F79FuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debugging algorithms with learning and validation curves"
      ],
      "metadata": {
        "id": "G8d2JVMK843b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "pipeline = make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', max_iter=1000))\n",
        "\n",
        "train_sizes, train_scores, test_scores =\\\n",
        "                          learning_curve(estimator=pipeline,X =X_train, y=y_train,\n",
        "                          train_sizes=np.linspace(0.1,1.0,10), cv=10)\n",
        "# Take the mean & std of the accuracies\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "plt.plot(train_sizes, train_mean,color='blue', marker='o',markersize=5, label='Training accuracy')\n",
        "plt.fill_between(train_sizes,train_mean + train_std,train_mean - train_std,alpha=0.15, color='blue')\n",
        "plt.plot(train_sizes, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='Validation accuracy')\n",
        "plt.fill_between(train_sizes,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n",
        "plt.grid()\n",
        "plt.xlabel('Number of training examples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "# set the y label axis units we want from 80% to 100%\n",
        "plt.ylim([0.8, 1.03])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XTrlojk-_kom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Printing out the Validation Curve**\n",
        "\n",
        "*Here we look for tweaking the parameter which is `C` from logistic regression, so we provide a desired range `param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]` then in the curve function we specified the parameter we want to tweak i.e. `'logisticregression__C'`*"
      ],
      "metadata": {
        "id": "qGpjrxewEzfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import validation_curve\n",
        "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "train_scores, test_scores = validation_curve(estimator=pipeline, X = X_train, y = y_train, param_name='logisticregression__C',\n",
        "                                  param_range=param_range,cv=10)\n",
        "# Take the mean & std of the accuracies\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "plt.plot(param_range, train_mean,color='blue', marker='o',markersize=5, label='Training accuracy')\n",
        "plt.fill_between(param_range,train_mean + train_std,train_mean - train_std,alpha=0.15, color='blue')\n",
        "plt.plot(param_range, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='Validation accuracy')\n",
        "plt.fill_between(param_range,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n",
        "plt.grid()\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Parameter C')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "# set the y label axis units we want from 80% to 100%\n",
        "plt.ylim([0.8, 1.03])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cuTUqMqHDfPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*From above validation curve we can interpret that Best value of C will be between 0.1 & 1.0(10^0)*"
      ],
      "metadata": {
        "id": "fTgHR28fFjGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tunning the Model using Hyperparameter grid search"
      ],
      "metadata": {
        "id": "ZJwCq-aiGE0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipe_svc = make_pipeline(StandardScaler(), SVC())\n",
        "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
        "\n",
        "param_grid = [{'svc__C': param_range, 'svc__kernel': ['linear']},\n",
        "              {'svc__C': param_range, 'svc__gamma': param_range, 'svc__kernel': ['rbf']}]\n",
        "\n",
        "gs = GridSearchCV(estimator=pipe_svc,\n",
        "                  param_grid=param_grid,\n",
        "                  scoring='accuracy',\n",
        "                  cv=10,\n",
        "                  refit=True)\n",
        "\n",
        "gs = gs.fit(X_train, y_train)\n",
        "\n",
        "#return the best score and best parameters\n",
        "print(gs.best_score_)\n",
        "print(gs.best_params_)"
      ],
      "metadata": {
        "id": "mIfV9dusGJUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Get the best model using  gs.`best_estimator_`*"
      ],
      "metadata": {
        "id": "_vdpYnhGG5Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = gs.best_estimator_\n",
        "# If we set refit=True, no need to fit the best_model again as grid search will\n",
        "# automatically set the best params if this attr. is set to true\n",
        "# best_model.fit(X_train, y_train)\n",
        "print(f\"Testing Accuracy {best_model.score(X_test,y_test)*100:.2f}\")"
      ],
      "metadata": {
        "id": "YYmXNi9BG3QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "WEQlYhTTqb2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Metrics"
      ],
      "metadata": {
        "id": "uWmbzVA3qH9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "\n",
        "pre_val = precision_score(y_true = y_test, y_pred = y_pred)\n",
        "\n",
        "rec_val = recall_score(y_true=y_test, y_pred = y_pred)\n",
        "\n",
        "f1_val = f1_score(y_true=y_test, y_pred = y_pred)\n",
        "\n",
        "mcc_val = matthews_corrcoef(y_true = y_test, y_pred = y_pred)\n",
        "\n",
        "roc_val = roc_auc_score(y_true = y_test, y_score = y_pred)\n",
        "\n",
        "print(f\"The Precision score: {pre_val} | the recall score is {rec_val} | the f1 score is {f1_val} | the mcc score is {mcc_val} | Roc value is {roc_val}\")"
      ],
      "metadata": {
        "id": "JX8Jnv64qKiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with Class Imbalance"
      ],
      "metadata": {
        "id": "rNHkPK49t947"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's create an imbalance dataset, we will pick only 40 samples of class 1 and all samples of class 0**"
      ],
      "metadata": {
        "id": "E8YM-hlHvVah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape[0])"
      ],
      "metadata": {
        "id": "Y6fQipAJvcXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_imb = np.vstack((X[y ==0],X[y == 1][:40]))\n",
        "y_imb = np.hstack((y[y == 0], y[y == 1][:40]))"
      ],
      "metadata": {
        "id": "VG2sndWgvgnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 212 class 0 samples in the dataset\n",
        "X_imb[y_imb == 0].shape[0]"
      ],
      "metadata": {
        "id": "qU--FH3xzFg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[y ==0],X[y == 1][:40]"
      ],
      "metadata": {
        "id": "YKlZA8Cjsffg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can see that, if we have all zeros as the prediction then, comparing it with the imbalance dataset gives us 85% which means that the dataset has 85% class label as 0 and the rest 15% as class label 1 which means our dataset is imbalance**"
      ],
      "metadata": {
        "id": "uoDfD5fzwlFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros(y_imb.shape[0])\n",
        "print(np.mean(y_pred == y_imb)*100)"
      ],
      "metadata": {
        "id": "fekiH3RvwPYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.ones(y_imb.shape[0])\n",
        "np.mean(y_pred == y_imb)*100"
      ],
      "metadata": {
        "id": "RIEtvcMqw4Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***As a result in such cases, the model will be more biased towards the majority class, and will fail to pick out any pattern in the dataset as most of the examples fall under class 0, the model will fail to learn anything hence we can't stick to accuracy or any other PM for the validation purpose***\n",
        "\n",
        "***The algorithm implicitly learns a model that optimizes the predictions based on the\n",
        "most abundant class in the dataset to minimize the loss or maximize the reward during training***"
      ],
      "metadata": {
        "id": "1jFHINqCxDOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*So for example if our focus was to identify the majority of patients with cancer, and assuming class 1 indicates a patient with cancer, then we would be looking at **Recall** as our performance metric as in recall, we try to maximise the TP*\n",
        "\n",
        "*In spam filtering, we don't want to label email as spam if system is not so sure, in that we must look for Precision*\n",
        "\n",
        "***Note:*** *A TP means that predicted and actual class/label was '1', a TN means that predicted and actual label was '0'. A FP means predicted class was '1' but actual was '0'. FN means predicted class '0' and actual class '1'*"
      ],
      "metadata": {
        "id": "OlBUFhL4xdn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty\n",
        "to wrong predictions on the minority class. Via scikit-learn, adjusting such a penalty is as convenient\n",
        "as setting the class_weight parameter to class_weight='balanced', which is implemented for most\n",
        "classifiers.***\n",
        "\n",
        "**Other popular strategies for dealing with class imbalance include upsampling the minority class,\n",
        "downsampling the majority class, and the generation of synthetic training examples**"
      ],
      "metadata": {
        "id": "1sMDBC2WyvYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "print(f\"Number of class 1 before resample {X_imb[y_imb == 1].shape[0]}\")\n",
        "\n",
        "X_upsampled, y_upsampled = resample(X_imb[y_imb == 1], y_imb[y_imb == 1], replace = True, n_samples = X_imb[y_imb == 0].shape[0])\n",
        "\n",
        "\n",
        "# So it matches with the number of samples of majority class in this case class 0, hence in resample we provide n_sample = the len of majority class samples\n",
        "print(f\"Number of class 1 after resample {X_upsampled[y_upsampled == 1].shape[0]}\")"
      ],
      "metadata": {
        "id": "z5KpQ6j4xV9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*After upsampling we have to stack the upsampled with the majority one as in if you check `y_upsampled[y_upsampled == 0]` you will get an empty result as class 0 is not present in the upsampled data*"
      ],
      "metadata": {
        "id": "Fmpj6mSH0heT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_balanced = np.vstack((X[y == 0], X_upsampled))\n",
        "y_balanced = np.hstack((y[y == 0], y_upsampled))"
      ],
      "metadata": {
        "id": "_wLcgFHi0vgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_balanced[y_balanced == 0].shape[0]"
      ],
      "metadata": {
        "id": "ylH03jw21wov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.ones(y_balanced.shape[0])\n",
        "\n",
        "np.mean(y_pred == y_balanced)*100\n"
      ],
      "metadata": {
        "id": "F7HFMbl10BSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Further Read: Synthetic Minority Over-sampling Technique (SMOTE) also highly recommended to check out imbalanced-learn, a Python library that is entirely focused on imbalanced datasets,including an implementation of SMOTE. You can learn more about imbalanced-learn at**\n",
        "https://github.com/scikit-learn-contrib/imbalanced-learn."
      ],
      "metadata": {
        "id": "ehi54h4d3JPH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0FWCgKI3TUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}